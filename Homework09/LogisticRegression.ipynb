{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "from math import e, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   class  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "data['class'] = iris.target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, alpha=0.01, n_iter=1000, threshold=0.5, l2=0.01, intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.l2 = l2\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.n_iter = n_iter\n",
    "        self.intercept = intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        precision = 0.001\n",
    "\n",
    "        if len(X.shape) == 1:\n",
    "            m, n = X.shape[0], 1\n",
    "        else:\n",
    "            m, n = X.shape\n",
    "\n",
    "        self.theta = np.full((n,), 0.33)\n",
    "        for j in range(self.n_iter):\n",
    "            theta_old = tuple(self.theta)\n",
    "\n",
    "            y_hypoth = X.dot(self.theta)\n",
    "            p = e ** y_hypoth / (e ** y_hypoth + 1)\n",
    "\n",
    "            self.grad = X.T.dot(p - y) / m\n",
    "            penalty = 2 * self.l2 * self.theta\n",
    "            if self.intercept:\n",
    "                penalty[0] = 0\n",
    "            self.grad += penalty\n",
    "\n",
    "#             check, J_plus, J_minus, diff = self.gradient_check(X, y)\n",
    "#             if not check:\n",
    "#                 print('gradient is incorrect!')\n",
    "#                 print(J_plus, J_minus, diff)\n",
    "#                 return\n",
    "\n",
    "            self.theta -= self.alpha * self.grad\n",
    "\n",
    "            precise = True\n",
    "            for i in range(n):\n",
    "                if abs(theta_old[i] - self.theta[i]) > precision:\n",
    "                    precise = False\n",
    "                    break\n",
    "            if precise:\n",
    "                print('iteration', str(j) + ': precise enough')\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_hypoth = X_test.dot(self.theta)\n",
    "        p = e ** y_hypoth / (e ** y_hypoth + 1)\n",
    "\n",
    "        get_predictions = lambda x: 1 if x > self.threshold else 0\n",
    "        return pd.Series(np.vectorize(get_predictions)(p)), p\n",
    "\n",
    "    def gradient_check(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        if len(X.shape) == 1:\n",
    "            m, n = X.shape[0], 1\n",
    "        else:\n",
    "            m, n = X.shape[0], X.shape[1]\n",
    "\n",
    "        epsilon = 10e-7\n",
    "\n",
    "        for j in range(len(self.grad)):\n",
    "            vector = np.full((n,), 0)\n",
    "            vector[j] = 1\n",
    "\n",
    "            theta_plus = self.theta + vector * epsilon\n",
    "            y_hypoth_plus = X.dot(theta_plus)\n",
    "            p_plus = e ** y_hypoth_plus / (e ** y_hypoth_plus + 1)\n",
    "            J_plus = - sum(y * np.log(p_plus) + (1 - y) * np.log(1 - p_plus)) / m + self.l2 * sum(theta_plus ** 2)\n",
    "\n",
    "            theta_minus = self.theta - vector * epsilon\n",
    "            y_hypoth_minus = X.dot(theta_minus)\n",
    "            p_minus = e ** y_hypoth_minus / (e ** y_hypoth_minus + 1)\n",
    "            J_minus = - sum(y * np.log(p_minus) + (1 - y) * np.log(1 - p_minus)) / m + self.l2 * sum(theta_minus ** 2)\n",
    "\n",
    "            diff = abs((J_plus - J_minus) / (2 * epsilon) - self.grad[j])\n",
    "            if diff > epsilon:\n",
    "                return False, J_plus, J_minus, diff\n",
    "\n",
    "        return True, J_plus, J_minus, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchLR:\n",
    "    def __init__(self, alpha=0.01, epochs=100, batch_size=10, threshold=0.5, l2=0.01, intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.l2 = l2\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.intercept = intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        precision = 0.001\n",
    "\n",
    "        if len(X.shape) == 1:\n",
    "            m, n = X.shape[0], 1\n",
    "        else:\n",
    "            m, n = X.shape\n",
    "\n",
    "        self.theta = np.full((n,), 0.33)\n",
    "        for j in range(self.epochs):\n",
    "            theta_old = tuple(self.theta)\n",
    "\n",
    "            for k in range(0, m, self.batch_size):\n",
    "                start = k\n",
    "                size = min(self.batch_size, m - k)\n",
    "                stop = k + size\n",
    "#                 print('start =', start, ' stop =', stop)\n",
    "\n",
    "                X_batch = X[start:stop]\n",
    "                y_batch = y[start:stop]\n",
    "\n",
    "                y_hypoth = X_batch.dot(self.theta)\n",
    "                p = e ** y_hypoth / (e ** y_hypoth + 1)\n",
    "\n",
    "                self.grad = X_batch.T.dot(p - y_batch) / size\n",
    "                penalty = 2 * self.l2 * self.theta\n",
    "\n",
    "                if self.intercept:\n",
    "                    penalty[0] = 0\n",
    "                self.grad += penalty\n",
    "                self.theta -= self.alpha * self.grad\n",
    "\n",
    "            precise = True\n",
    "            for i in range(n):\n",
    "                if abs(theta_old[i] - self.theta[i]) > precision:\n",
    "                    precise = False\n",
    "                    break\n",
    "            if precise:\n",
    "                print('epoch', str(j) + ': precise enough')\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_hypoth = X_test.dot(self.theta)\n",
    "        p = e ** y_hypoth / (e ** y_hypoth + 1)\n",
    "\n",
    "        get_predictions = lambda x: 1 if x > self.threshold else 0\n",
    "        return pd.Series(np.vectorize(get_predictions)(p)), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_hypoth, y):\n",
    "    y_hypoth, y = np.array(y_hypoth), np.array(y)\n",
    "    tp = ((y_hypoth == y) & (y == 1)).sum()\n",
    "    fp = ((y == 0) & (y_hypoth == 1)).sum()\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_hypoth, y):\n",
    "    y_hypoth, y = np.array(y_hypoth), np.array(y)\n",
    "    tp = ((y_hypoth == y) & (y == 1)).sum()\n",
    "    fn = ((y_hypoth == 0) & (y == 1)).sum()\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_measure(y_hypoth, y):\n",
    "    pr, rec = precision(y_hypoth, y), recall(y_hypoth, y)\n",
    "    return 2 * pr * rec / (pr + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max(X):\n",
    "    if len(X.shape) == 1:\n",
    "        X = [(x - min(X)) / (max(X) - min(X)) for x in X]\n",
    "        return\n",
    "    \n",
    "    for col in X.columns:\n",
    "        if max(X[col]) == min(X[col]):\n",
    "            X[col] = 1\n",
    "        else:\n",
    "            X[col] = [(x - min(X[col])) / (max(X[col]) - min(X[col])) for x in X[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop('class', axis=1), data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_two_class, y_two_class = data[data['class'] <= 1].drop('class', axis=1), data[data['class'] <= 1]['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>sepal length (cm)^2</th>\n",
       "      <th>sepal length (cm) sepal width (cm)</th>\n",
       "      <th>sepal length (cm) petal length (cm)</th>\n",
       "      <th>sepal length (cm) petal width (cm)</th>\n",
       "      <th>sepal width (cm)^2</th>\n",
       "      <th>sepal width (cm) petal length (cm)</th>\n",
       "      <th>sepal width (cm) petal width (cm)</th>\n",
       "      <th>petal length (cm)^2</th>\n",
       "      <th>petal length (cm) petal width (cm)</th>\n",
       "      <th>petal width (cm)^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>26.01</td>\n",
       "      <td>17.85</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1.02</td>\n",
       "      <td>12.25</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>24.01</td>\n",
       "      <td>14.70</td>\n",
       "      <td>6.86</td>\n",
       "      <td>0.98</td>\n",
       "      <td>9.00</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>22.09</td>\n",
       "      <td>15.04</td>\n",
       "      <td>6.11</td>\n",
       "      <td>0.94</td>\n",
       "      <td>10.24</td>\n",
       "      <td>4.16</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>21.16</td>\n",
       "      <td>14.26</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.92</td>\n",
       "      <td>9.61</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>25.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>12.96</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "0  1.0                5.1               3.5                1.4   \n",
       "1  1.0                4.9               3.0                1.4   \n",
       "2  1.0                4.7               3.2                1.3   \n",
       "3  1.0                4.6               3.1                1.5   \n",
       "4  1.0                5.0               3.6                1.4   \n",
       "\n",
       "   petal width (cm)  sepal length (cm)^2  sepal length (cm) sepal width (cm)  \\\n",
       "0               0.2                26.01                               17.85   \n",
       "1               0.2                24.01                               14.70   \n",
       "2               0.2                22.09                               15.04   \n",
       "3               0.2                21.16                               14.26   \n",
       "4               0.2                25.00                               18.00   \n",
       "\n",
       "   sepal length (cm) petal length (cm)  sepal length (cm) petal width (cm)  \\\n",
       "0                                 7.14                                1.02   \n",
       "1                                 6.86                                0.98   \n",
       "2                                 6.11                                0.94   \n",
       "3                                 6.90                                0.92   \n",
       "4                                 7.00                                1.00   \n",
       "\n",
       "   sepal width (cm)^2  sepal width (cm) petal length (cm)  \\\n",
       "0               12.25                                4.90   \n",
       "1                9.00                                4.20   \n",
       "2               10.24                                4.16   \n",
       "3                9.61                                4.65   \n",
       "4               12.96                                5.04   \n",
       "\n",
       "   sepal width (cm) petal width (cm)  petal length (cm)^2  \\\n",
       "0                               0.70                 1.96   \n",
       "1                               0.60                 1.96   \n",
       "2                               0.64                 1.69   \n",
       "3                               0.62                 2.25   \n",
       "4                               0.72                 1.96   \n",
       "\n",
       "   petal length (cm) petal width (cm)  petal width (cm)^2  \n",
       "0                                0.28                0.04  \n",
       "1                                0.28                0.04  \n",
       "2                                0.26                0.04  \n",
       "3                                0.30                0.04  \n",
       "4                                0.28                0.04  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2, include_bias=True)\n",
    "X_two_class_poly = pd.DataFrame(data = poly.fit_transform(X_two_class), columns=poly.get_feature_names(X_two_class.columns))\n",
    "X_two_class_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max(X_two_class_poly)\n",
    "min_max(y_two_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 507: precise enough\n",
      "Wall time: 135 ms\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_two_class_poly, y_two_class, test_size=0.33, random_state=18)\n",
    "# model = LogisticRegression(alpha=0.01, threshold=0.5, l2=0.01, intercept=True)\n",
    "\n",
    "model = MiniBatchLR(alpha=0.01, epochs=10000, batch_size=10, threshold=0.5, l2=0.01, intercept=True)\n",
    "%time model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hypoth, pred_proba = model.predict(X_test)\n",
    "precision(Y_hypoth, Y_test), recall(Y_hypoth, Y_test), f_measure(Y_hypoth, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.52611646,  0.20739539, -0.8846714 ,  0.84175319,  0.84246498,\n",
       "        0.23901378, -0.51966476,  0.75169732,  0.78547712, -0.77729013,\n",
       "        0.58341294,  0.66333183,  0.83816077,  0.84164902,  0.75135035])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x552e190>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_proba.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, alpha=0.01, epochs=100, batch_size=10, threshold=0.5, l2=0.01, intercept=False):\n",
    "        self.alpha = alpha\n",
    "        self.l2 = l2\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.intercept = intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        precision = 0.001\n",
    "\n",
    "        if len(X.shape) == 1:\n",
    "            M, N = X.shape[0], 1\n",
    "        else:\n",
    "            M, N = X.shape\n",
    "        K = pd.Series(y).nunique()\n",
    "\n",
    "        self.theta = np.array([[0.33] * N for _ in range(K)])\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            if epoch % 10 == 0:\n",
    "                print('epoch', epoch)\n",
    "            \n",
    "            theta_old = [tuple(self.theta[j]) for j in range(K)]\n",
    "\n",
    "            for b in range(0, M, self.batch_size):\n",
    "                start = b\n",
    "                size = min(self.batch_size, M - b)\n",
    "                stop = b + size\n",
    "\n",
    "                X_batch = X[start:stop]\n",
    "                y_batch = y[start:stop]\n",
    "\n",
    "                y_hypoth = X_batch.dot(self.theta.T)\n",
    "                p = e ** y_hypoth\n",
    "                for i in range(size):\n",
    "                    p[i] /= sum(e ** y_hypoth[i])\n",
    "\n",
    "                self.grad = np.array([[0.0] * N for j in range(K)])\n",
    "                for j in range(K):\n",
    "                    for n in range(N):\n",
    "                        for i in range(size):\n",
    "                            if j == y_batch[i]:\n",
    "                                self.grad[j][n] += (p[i][j] - 1.0) * X_batch[i][n]\n",
    "                            else:\n",
    "                                self.grad[j][n] += p[i][j] * X_batch[i][n]\n",
    "                \n",
    "                penalty = 2 * self.l2 * self.theta\n",
    "                if self.intercept:\n",
    "                    penalty[:, 0] = 0.0\n",
    "                self.grad += penalty\n",
    "                \n",
    "                self.theta -= self.alpha * self.grad\n",
    "\n",
    "            precise = True\n",
    "            for j in range(K):\n",
    "                for n in range(N):\n",
    "                    if abs(theta_old[j][n] - self.theta[j][n]) > precision:\n",
    "                        precise = False\n",
    "                        break\n",
    "            if precise:\n",
    "                print('epoch', str(epoch) + ': precise enough')\n",
    "                break\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        \n",
    "        y_hypoth = X_test.dot(self.theta.T)        \n",
    "        p = e ** y_hypoth\n",
    "        for i in range(len(X_test)):\n",
    "            p[i] /= sum(e ** y_hypoth[i])\n",
    "        \n",
    "        predictions = [p[i].argmax() for i in range(len(X_test))]\n",
    "        pred_proba = [max(p[i]) for i in range(len(X_test))]\n",
    "        return pd.Series(predictions), pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_hypoth, y_test):\n",
    "    y_hypoth, y_test = np.array(y_hypoth), np.array(y_test)\n",
    "    classes = pd.Series(y_test).unique()\n",
    "    K = len(classes)\n",
    "    \n",
    "    confusion_matrix = [[0] * K for _ in range(K)]\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            confusion_matrix[i][j] = ((y_test == i) & (y_hypoth == j)).sum()\n",
    "    \n",
    "    tp, fp, fn = np.full((K,), 0), np.full((K,), 0), np.full((K,), 0)\n",
    "    for j in range(K):\n",
    "        tp[j] = confusion_matrix[j][j]\n",
    "        fp[j] = sum([confusion_matrix[i][j] for i in range(K)]) - tp[j]\n",
    "        fn[j] = sum(confusion_matrix[j]) - tp[j]\n",
    "    \n",
    "    pr_micro = sum(tp) / sum(tp + fp)\n",
    "    r_micro = sum(tp) / sum(tp + fn)\n",
    "    f_micro = 2 * pr_micro * r_micro / (pr_micro + r_micro)\n",
    "    \n",
    "    pr_macro = sum(tp / (tp + fp)) / K\n",
    "    r_macro = sum(tp / (tp + fn)) / K\n",
    "    f_macro = 2 * pr_macro * r_macro / (pr_macro + r_macro)\n",
    "    \n",
    "    return f_micro, f_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = pd.DataFrame(data = poly.fit_transform(X), columns=poly.get_feature_names(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max(X_poly)\n",
    "min_max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 10\n",
      "epoch 20\n",
      "epoch 30\n",
      "epoch 40\n",
      "epoch 50\n",
      "epoch 60\n",
      "epoch 70\n",
      "epoch 80\n",
      "epoch 90\n",
      "epoch 100\n",
      "epoch 110\n",
      "epoch 120\n",
      "epoch 130\n",
      "epoch 140\n",
      "epoch 150\n",
      "epoch 160\n",
      "epoch 170\n",
      "epoch 180\n",
      "epoch 190\n",
      "epoch 200\n",
      "epoch 210\n",
      "epoch 220\n",
      "epoch 230\n",
      "epoch 240\n",
      "epoch 250\n",
      "epoch 260\n",
      "epoch 270\n",
      "epoch 280\n",
      "epoch 290\n",
      "epoch 300\n",
      "epoch 310\n",
      "epoch 320\n",
      "epoch 330\n",
      "epoch 340\n",
      "epoch 350\n",
      "epoch 360\n",
      "epoch 370\n",
      "epoch 380\n",
      "epoch 390\n",
      "epoch 397: precise enough\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=18)\n",
    "model = SoftmaxRegression(alpha=0.01, epochs=1000, batch_size=10, threshold=0.5, l2=0.01, intercept=True)\n",
    "%time model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dde5f10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_hypoth, pred_proba = model.predict(X_test)\n",
    "pd.Series(pred_proba).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.98, 0.98080864542575)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(Y_hypoth, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2     3     4     5    6    7    8    9  ...   55   56   57  \\\n",
       "0  0.0  0.0  5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3  0.0  0.0  7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  0.0  0.0  0.0   \n",
       "4  0.0  0.0  0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "    58    59    60    61   62   63  class  \n",
       "0  6.0  13.0  10.0   0.0  0.0  0.0      0  \n",
       "1  0.0  11.0  16.0  10.0  0.0  0.0      1  \n",
       "2  0.0   3.0  11.0  16.0  9.0  0.0      2  \n",
       "3  7.0  13.0  13.0   9.0  0.0  0.0      3  \n",
       "4  0.0   2.0  16.0   4.0  0.0  0.0      4  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits()\n",
    "data = pd.DataFrame(data=mnist.data)\n",
    "data['class'] = mnist.target\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 65)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data, pd.Series(mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 10\n",
      "epoch 20\n",
      "epoch 30\n",
      "epoch 40\n",
      "epoch 50\n",
      "epoch 60\n",
      "epoch 70\n",
      "epoch 80\n",
      "epoch 90\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=18)\n",
    "model = SoftmaxRegression(alpha=0.01, epochs=100, batch_size=25, threshold=0.5, l2=0.1, intercept=True)\n",
    "%time model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e04c0f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y_hypoth, pred_proba = model.predict(X_test)\n",
    "pd.Series(pred_proba).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.968013468013468, 0.9698072192089193)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(Y_hypoth, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
